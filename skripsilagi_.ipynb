{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiara070403/skripsi/blob/main/skripsilagi_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMUmyHLxTPXg",
        "outputId": "6670d381-98b8-4de3-bc61-8ea650defba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn pandas matplotlib seaborn Sastrawi transformers wordcloud nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.pipeline import Pipeline\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from transformers import pipeline\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/crawling.csv', index_col=0)"
      ],
      "metadata": {
        "id": "lRjq11UeVWxi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy = df.copy()"
      ],
      "metadata": {
        "id": "V1FHaiZNZbX2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['created_at', 'full_text']]"
      ],
      "metadata": {
        "id": "F9GG4pj1VmkP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "0QGG0mTHVuD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "872f0f4a-56df-4418-b66a-a09e6abe0a68"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 5349 entries, 1858070162362204643 to 1230765438897815552\n",
            "Data columns (total 2 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   created_at  5349 non-null   object\n",
            " 1   full_text   5349 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 125.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengubah teks menjadi lowercase\n",
        "df['full_text'] = df['full_text'].astype(str).str.lower()\n",
        "df['full_text'] = df['full_text'].replace('false', pd.NA)\n",
        "df = df.dropna(subset=['full_text'])"
      ],
      "metadata": {
        "id": "x7E237CWFhi2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghapus duplikat\n",
        "initial_duplicates = df.duplicated().sum()\n",
        "df = df.drop_duplicates(subset=['full_text'])\n",
        "print(f\"Jumlah duplikat awal: {initial_duplicates}\")\n",
        "print(f\"Jumlah duplikat setelah drop: {df.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "IllVA0F5Vwqh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4337e843-12da-41e0-c4d2-72ee54aaabff"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah duplikat awal: 4\n",
            "Jumlah duplikat setelah drop: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghapus baris dengan nilai null (setelah dropna subset)\n",
        "df = df.dropna()\n",
        "print(f\"Jumlah nilai null setelah dropna: \\n{df.isnull().sum()}\")"
      ],
      "metadata": {
        "id": "5WeD8UkzV1qH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01779a47-946b-4452-aad9-043d72fdd42b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah nilai null setelah dropna: \n",
            "created_at    0\n",
            "full_text     0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Case Folding\n",
        "df['full_text'] = df['full_text'].str.lower()\n",
        "print(\"\\nDataFrame setelah Case Folding:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "9G2F9u6rWCtX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b83c1ea6-a188-49b3-ae39-4ea6ab469d2e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame setelah Case Folding:\n",
            "                                         created_at  \\\n",
            "conversation_id_str                                   \n",
            "1858070162362204643  Sun Nov 17 08:50:22 +0000 2024   \n",
            "1846529554657239219  Wed Oct 16 12:32:07 +0000 2024   \n",
            "1467691301155979267  Wed Oct 09 13:58:44 +0000 2024   \n",
            "1836014724523458593  Tue Sep 17 12:09:56 +0000 2024   \n",
            "1835164951125917967  Sun Sep 15 03:53:14 +0000 2024   \n",
            "\n",
            "                                                             full_text  \n",
            "conversation_id_str                                                     \n",
            "1858070162362204643            lucu banget mahasiswa pencinta alam ini  \n",
            "1846529554657239219  bejat mahasiswa pencinta alam di jambi 'genjot...  \n",
            "1467691301155979267  @leonita_lestari ada perkoempoelan pentjinta a...  \n",
            "1836014724523458593  [press release sowan ukm ke ukm unit pandu lin...  \n",
            "1835164951125917967  [press release sowan ukm ke ukm unit pandu lin...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning\n",
        "def clean_twitter_text(text):\n",
        "  text = re.sub(r'@[A-Za-z0-9_]+', '', text) # Menghapus mention\n",
        "  text = re.sub(r'#\\w+', '', text)          # Menghapus hashtag\n",
        "  text = re.sub(r'RT[\\s]+', '', text)        # Menghapus RT\n",
        "  text = re.sub(r'https?://\\S+', '', text)   # Menghapus URL\n",
        "  text = re.sub(r'[^A-Za-z0-9 ]', '', text)  # Menghapus karakter non-alfanumerik kecuali spasi\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()   # Menghapus spasi berlebih\n",
        "  return text\n",
        "df['full_text'] = df['full_text'].apply(clean_twitter_text)\n",
        "print(f\"\\nShape setelah Cleaning: {df.shape}\")\n",
        "print(\"DataFrame setelah Cleaning:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "yPyxz9vjWFt-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d64c5de-f9a8-4708-833a-9ace13d1dae1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shape setelah Cleaning: (5246, 2)\n",
            "DataFrame setelah Cleaning:\n",
            "                                         created_at  \\\n",
            "conversation_id_str                                   \n",
            "1858070162362204643  Sun Nov 17 08:50:22 +0000 2024   \n",
            "1846529554657239219  Wed Oct 16 12:32:07 +0000 2024   \n",
            "1467691301155979267  Wed Oct 09 13:58:44 +0000 2024   \n",
            "1836014724523458593  Tue Sep 17 12:09:56 +0000 2024   \n",
            "1835164951125917967  Sun Sep 15 03:53:14 +0000 2024   \n",
            "\n",
            "                                                             full_text  \n",
            "conversation_id_str                                                     \n",
            "1858070162362204643            lucu banget mahasiswa pencinta alam ini  \n",
            "1846529554657239219  bejat mahasiswa pencinta alam di jambi genjot ...  \n",
            "1467691301155979267  ada perkoempoelan pentjinta alam yang lahir th...  \n",
            "1836014724523458593  press release sowan ukm ke ukm unit pandu ling...  \n",
            "1835164951125917967  press release sowan ukm ke ukm unit pandu ling...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter Token Length\n",
        "def filter_tokens_by_length(df, column, min_words, max_words):\n",
        "    words_count = df[column].astype(str).apply(lambda x: len(x.split()))\n",
        "    mask = (words_count >= min_words) & (words_count <= max_words)\n",
        "    filtered_df = df[mask]\n",
        "    return filtered_df\n",
        "min_words = 3\n",
        "max_words = 5500\n",
        "df = filter_tokens_by_length(df, 'full_text', min_words, max_words)\n",
        "print(f\"\\nShape setelah Filter Token Length: {df.shape}\")\n",
        "print(\"DataFrame setelah Filter Token Length:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "bnbK-nBzFv7G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9218e211-68f3-42de-ae0a-e15543df9b4d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shape setelah Filter Token Length: (5170, 2)\n",
            "DataFrame setelah Filter Token Length:\n",
            "                                         created_at  \\\n",
            "conversation_id_str                                   \n",
            "1858070162362204643  Sun Nov 17 08:50:22 +0000 2024   \n",
            "1846529554657239219  Wed Oct 16 12:32:07 +0000 2024   \n",
            "1467691301155979267  Wed Oct 09 13:58:44 +0000 2024   \n",
            "1836014724523458593  Tue Sep 17 12:09:56 +0000 2024   \n",
            "1835164951125917967  Sun Sep 15 03:53:14 +0000 2024   \n",
            "\n",
            "                                                             full_text  \n",
            "conversation_id_str                                                     \n",
            "1858070162362204643            lucu banget mahasiswa pencinta alam ini  \n",
            "1846529554657239219  bejat mahasiswa pencinta alam di jambi genjot ...  \n",
            "1467691301155979267  ada perkoempoelan pentjinta alam yang lahir th...  \n",
            "1836014724523458593  press release sowan ukm ke ukm unit pandu ling...  \n",
            "1835164951125917967  press release sowan ukm ke ukm unit pandu ling...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalisasi Data\n",
        "norm = {'kekekekegiatanananan': 'kegiatan', 'kamucu': 'kamu', 'jutidak': 'tidak', 'lingkung': 'lingkungan',\n",
        "    'berberberbersama': 'bersama', 'press release': 'siaran pers', 'ukamu': 'ukm',\n",
        "    'kekekekegiatanananan': 'kegiatan', 'jutidak': 'tidak', 'lingkung': 'lingkungan',\n",
        "    'berberberbersama': 'bersama', 'press release': 'siaran pers', 'ukamu': 'ukm',\n",
        "    'giat': 'kegiatan', 'luring': 'luring', 'sekre': 'sekretariat',\n",
        "    'mapala': 'mahasiswa pencinta alam', 'kocak': 'lucu', 'enak': 'enak',\n",
        "    'bilang': 'bilang', 'sowan': 'mengunjungi', 'buka': 'membuka',\n",
        "    'sampai': 'hingga', 'pukul': 'jam', 'sd': 'sampai', 'kapan': 'kapan',\n",
        "    'mau': 'ingin', 'join': 'bergabung', 'sama': 'bersama', 'bisa': 'dapat',\n",
        "    'bantu': 'membantu', 'mikir2': 'mikir', 'perkoempoelan': 'perkumpulan',\n",
        "    'genjot': 'menyerang', 'wkwkw': 'hahaha', 'th': 'tahun', 'bejat': 'rusak',\n",
        "    'pentjinta': 'pencinta', 'siaaapp': 'siap', 'okaaay': 'oke', 'udh': 'sudah',\n",
        "    'ga': 'tidak', 'gaskeun': 'ayo', 'wowww': 'wow', 'haaayyuukkk': 'ayo',\n",
        "    'yg': 'yang', 'wkwk': '', 'min': 'kak', 'malem': 'malam', 'malem2': 'malam',\n",
        "    'sm': 'sama', 'dy': 'dia', 'lg': 'lagi', 'skrg': 'sekarang', 'ddpn': 'didepan',\n",
        "    'makasi': 'makasih', 'pertamaz': 'pertamax', 'jg': 'juga', 'donk': 'dong',\n",
        "    'ikutann': 'ikutan', 'banyakk': 'banyak', 'twt': 'tweet', 'mantaap': 'mantap',\n",
        "    'juarak': 'juara', 'daridulu': 'dari dulu', 'siapp': 'siap', 'gamau': 'tidak mau',\n",
        "    'sll': 'selalu', 'qu': 'aku', 'krn': 'karena', 'irii': 'iri', 'muluu': 'terus',\n",
        "    'mada': 'masa', 'jgn': 'jangan', 'muluuu': 'terus', 'ntar': 'nanti',\n",
        "    'awtnya': 'awetnya', 'gg': 'keren', 'kerennn': 'keren', 'bisaa': 'bisa',\n",
        "    'gaaa': 'tidak', 'nyampe': 'sampai', 'lu': 'kamu', 'ikhlaaasss': 'ikhlas',\n",
        "    'gak': 'tidak', 'klo': 'kalo', 'kyk': 'seperti', 'sbg': 'sebagai',\n",
        "    'giat': 'kegiatan', 'luring': 'luring', 'sekre': 'sekretariat',\n",
        "    'mapala': 'mahasiswa pencinta alam', 'kocak': 'lucu', 'enak': 'enak',\n",
        "    'bilang': 'bilang', 'sowan': 'mengunjungi', 'buka': 'membuka',\n",
        "    'sampai': 'hingga', 'pukul': 'jam', 'sd': 'sampai', 'kapan': 'kapan',\n",
        "    'mau': 'ingin', 'join': 'bergabung', 'sama': 'bersama', 'bisa': 'dapat',\n",
        "    'bantu': 'membantu', 'mikir2': 'mikir', 'perkoempoelan': 'perkumpulan',\n",
        "    'genjot': 'menyerang', 'wkwkw': 'hahaha', 'th': 'tahun', 'bejat': 'rusak',\n",
        "    'pentjinta': 'pencinta', 'siaaapp': 'siap', 'okaaay': 'oke', 'udh': 'sudah',\n",
        "    'ga': 'tidak', 'gaskeun': 'ayo', 'wowww': 'wow', 'haaayyuukkk': 'ayo',\n",
        "    'yg': 'yang', 'wkwk': '', 'min': 'kak', 'malem': 'malam', 'malem2': 'malam',\n",
        "    'sm': 'sama', 'dy': 'dia', 'lg': 'lagi', 'skrg': 'sekarang', 'ddpn': 'didepan',\n",
        "    'makasi': 'makasih', 'pertamaz': 'pertamax', 'jg': 'juga', 'donk': 'dong',\n",
        "    'ikutann': 'ikutan', 'banyakk': 'banyak', 'twt': 'tweet', 'mantaap': 'mantap',\n",
        "    'juarak': 'juara', 'daridulu': 'dari dulu', 'siapp': 'siap', 'gamau': 'tidak mau',\n",
        "    'sll': 'selalu', 'qu': 'aku', 'krn': 'karena', 'irii': 'iri', 'muluu': 'terus',\n",
        "    'mada': 'masa', 'jgn': 'jangan', 'muluuu': 'terus', 'ntar': 'nanti',\n",
        "    'awtnya': 'awetnya', 'gg': 'keren', 'kerennn': 'keren', 'bisaa': 'bisa',\n",
        "    'gaaa': 'tidak', 'nyampe': 'sampai', 'lu': 'kamu', 'ikhlaaasss': 'ikhlas',\n",
        "    'gak': 'tidak', 'klo': 'kalo', 'kyk': 'seperti', 'sbg': 'sebagai'}\n",
        "def normalisasi(str_text):\n",
        "  for i in norm:\n",
        "    str_text = str_text.replace(i, norm[i])\n",
        "  return str_text\n",
        "df['full_text'] = df['full_text'].apply(lambda x: normalisasi(x))\n",
        "print(\"\\nDataFrame setelah Normalisasi:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "D1xqPx6sWXpD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecdbca09-2d97-404a-cb09-7f3432cba44f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame setelah Normalisasi:\n",
            "                                         created_at  \\\n",
            "conversation_id_str                                   \n",
            "1858070162362204643  Sun Nov 17 08:50:22 +0000 2024   \n",
            "1846529554657239219  Wed Oct 16 12:32:07 +0000 2024   \n",
            "1467691301155979267  Wed Oct 09 13:58:44 +0000 2024   \n",
            "1836014724523458593  Tue Sep 17 12:09:56 +0000 2024   \n",
            "1835164951125917967  Sun Sep 15 03:53:14 +0000 2024   \n",
            "\n",
            "                                                             full_text  \n",
            "conversation_id_str                                                     \n",
            "1858070162362204643          kamucu banget mahasiswa pencinta alam ini  \n",
            "1846529554657239219  rusak mahasiswa pencinta alam di jambi menyera...  \n",
            "1467691301155979267  ada perkumpulan pencinta alam yang lahir tahun...  \n",
            "1836014724523458593  siaran pers mengunjungi ukm ke ukm unit pandu ...  \n",
            "1835164951125917967  siaran pers mengunjungi ukm ke ukm unit pandu ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords Removal\n",
        "more_stop_words = ['anjayyy', 'gtgtgt']\n",
        "stop_words = StopWordRemoverFactory().get_stop_words()\n",
        "stop_words.extend(more_stop_words)\n",
        "new_array = ArrayDictionary(stop_words)\n",
        "stop_words_remover_new = StopWordRemover(new_array)\n",
        "def stopword(str_text):\n",
        "  str_text = stop_words_remover_new.remove(str_text)\n",
        "  return str_text\n",
        "df['full_text'] = df['full_text'].apply(lambda x: stopword(x))\n",
        "print(\"\\nDataFrame setelah Stopwords Removal:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "YrX60CN_WZJk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96b0e5a4-da38-4961-9dd2-8e96bf1a1611"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame setelah Stopwords Removal:\n",
            "                                         created_at  \\\n",
            "conversation_id_str                                   \n",
            "1858070162362204643  Sun Nov 17 08:50:22 +0000 2024   \n",
            "1846529554657239219  Wed Oct 16 12:32:07 +0000 2024   \n",
            "1467691301155979267  Wed Oct 09 13:58:44 +0000 2024   \n",
            "1836014724523458593  Tue Sep 17 12:09:56 +0000 2024   \n",
            "1835164951125917967  Sun Sep 15 03:53:14 +0000 2024   \n",
            "\n",
            "                                                             full_text  \n",
            "conversation_id_str                                                     \n",
            "1858070162362204643              kamucu banget mahasiswa pencinta alam  \n",
            "1846529554657239219  rusak mahasiswa pencinta alam jambi menyerang ...  \n",
            "1467691301155979267  perkumpulan pencinta alam lahir tahun 1953 did...  \n",
            "1836014724523458593  siaran pers mengunjungi ukm ukm unit pandu lin...  \n",
            "1835164951125917967  siaran pers mengunjungi ukm ukm unit pandu lin...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenisasi\n",
        "tokenized_for_stemming = df['full_text'].apply(lambda x: x.split())\n",
        "print(\"\\nTokenisasi (sebelum Stemming):\")\n",
        "print(tokenized_for_stemming.head())"
      ],
      "metadata": {
        "id": "ZgP35uTv7tNW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b0db96-cf4c-4522-e009-43acc9d05ad7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenisasi (sebelum Stemming):\n",
            "conversation_id_str\n",
            "1858070162362204643          [kamucu, banget, mahasiswa, pencinta, alam]\n",
            "1846529554657239219    [rusak, mahasiswa, pencinta, alam, jambi, meny...\n",
            "1467691301155979267    [perkumpulan, pencinta, alam, lahir, tahun, 19...\n",
            "1836014724523458593    [siaran, pers, mengunjungi, ukm, ukm, unit, pa...\n",
            "1835164951125917967    [siaran, pers, mengunjungi, ukm, ukm, unit, pa...\n",
            "Name: full_text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "def stemming_text(list_of_tokens):\n",
        "  stemmed_tokens = [stemmer.stem(w) for w in list_of_tokens]\n",
        "  return \" \".join(stemmed_tokens)\n",
        "df['full_text'] = tokenized_for_stemming.apply(stemming_text)\n",
        "print(\"\\nDataFrame setelah Stemming:\")\n",
        "df.to_csv('/content/stemmingg.csv', index=False)\n",
        "print(df.head())\n",
        "print(f\"Final DataFrame Shape setelah preprocessing: {df.shape}\")"
      ],
      "metadata": {
        "id": "pmB3KdSWxWnF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "656854fd-883d-4305-f08a-0170c3562e0d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3545828696.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mstemmed_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_for_stemming\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemming_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDataFrame setelah Stemming:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/stemmingg.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3545828696.py\u001b[0m in \u001b[0;36mstemming_text\u001b[0;34m(list_of_tokens)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_stemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstemming_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mstemmed_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_for_stemming\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemming_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3545828696.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_stemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstemming_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mstemmed_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_for_stemming\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemming_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/Sastrawi/Stemmer/CachedStemmer.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mstems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelegatedStemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mstems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/Sastrawi/Stemmer/Stemmer.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mstems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/Sastrawi/Stemmer/Stemmer.py\u001b[0m in \u001b[0;36mstem_word\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem_plural_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem_singular_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_plural\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/Sastrawi/Stemmer/Stemmer.py\u001b[0m in \u001b[0;36mstem_singular_word\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;34m\"\"\"Stem a singular word to its common stem form.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisitor_provider\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/Sastrawi/Stemmer/Context/Context.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#step 1 - 5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_stemming_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m#step 6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/Sastrawi/Stemmer/Context/Context.py\u001b[0m in \u001b[0;36mstart_stemming_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m#step 4, 5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_prefixes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/Sastrawi/Stemmer/Context/Context.py\u001b[0m in \u001b[0;36mremove_prefixes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mremove_prefixes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccept_prefix_visitors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix_pisitors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/Sastrawi/Stemmer/Context/Context.py\u001b[0m in \u001b[0;36maccept_prefix_visitors\u001b[0;34m(self, visitors)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_is_stopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremovals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mremovalCount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nDataframe setelah preprocessing disimpan ke /content/stemmingg.csv\")\n",
        "print(f\"Final DataFrame Shape setelah preprocessing: {df.shape}\")"
      ],
      "metadata": {
        "id": "h4V4nQRC5_MB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b186b148-1e12-4559-c9d1-f4e1382be204"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataframe setelah preprocessing disimpan ke /content/stemmingg.csv\n",
            "Final DataFrame Shape setelah preprocessing: (5170, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Labeling\n",
        "print(\"\\nLabeling\")\n",
        "df = pd.read_csv('/content/stemming.csv')\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"w11wo/indonesian-roberta-base-sentiment-classifier\")\n",
        "def prediksi_sentimen(teks):\n",
        "    if not isinstance(teks, str) or not teks.strip():\n",
        "        return 'neutral'\n",
        "    try:\n",
        "        hasil = classifier(teks)\n",
        "        return hasil[0]['label']\n",
        "    except Exception as e:\n",
        "        print(f\"Error predicting sentiment for text: '{teks[:50]}...' - {e}\")\n",
        "        return 'neutral'\n",
        "print(\"\\nMemulai proses Labeling\")\n",
        "df['sentimen'] = df['full_text'].apply(prediksi_sentimen)\n",
        "print(\"Labeling selesai.\")\n",
        "print(\"\\nDataframe dengan sentimen disimpan ke /content/labelinggg.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tEx2Np1C3d4",
        "outputId": "844cdc71-ac34-4b70-8f8a-9cc3bbaa3505"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Labeling\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Memulai proses Labeling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n/content/labelinggg.csv\")"
      ],
      "metadata": {
        "id": "_ar1yGpoDinA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentimen_counts = df.sentimen.value_counts()\n",
        "print(\"\\nDistribusi Sentimen:\")\n",
        "print(sentimen_counts)\n",
        "print(f\"\\nFinal DataFrame Shape: {df.shape}\")"
      ],
      "metadata": {
        "id": "7bNw5vZp492w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisasi\n",
        "print(\"\\nVisualisasi\")\n",
        "sns.set_palette(\"pastel\")\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='sentimen', data=df)\n",
        "plt.title('Analisis Sentimen Publik di X Terhadap Peran Mahasiswa Pencinta Alam dalam Pelestarian Lingkungan')\n",
        "plt.xlabel('Sentimen')\n",
        "plt.ylabel('Jumlah')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z-PhITyraCV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_positif = df[df['sentimen'] == 'positive']\n",
        "data_negatif = df[df['sentimen'] == 'negative']\n",
        "data_netral = df[df['sentimen'] == 'neutral']"
      ],
      "metadata": {
        "id": "xPHQR9p_Oeeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Cloud Positif\n",
        "if not data_positif.empty:\n",
        "    all_text_s1 = ' '.join(word for word in data_positif[\"full_text\"].dropna())\n",
        "    if all_text_s1:\n",
        "        wordcloud = WordCloud(colormap='Blues', width=1000, height=1000, mode=\"RGBA\", background_color='white').generate(all_text_s1)\n",
        "        plt.figure(figsize=(9, 6))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(\"Visualisasi Kata Positif\")\n",
        "        plt.margins(x=0, y=0)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "sYGzjYkbaMOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Cloud Negatif\n",
        "if not data_negatif.empty:\n",
        "    all_text_s0 = ' '.join(word for word in data_negatif[\"full_text\"].dropna())\n",
        "    if all_text_s0:\n",
        "        wordcloud = WordCloud(colormap='Reds', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text_s0)\n",
        "        plt.figure(figsize=(9, 6))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title(\"Visualisasi Kata Negatif\")\n",
        "        plt.margins(x=0, y=0)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "Btj3O5cYaP1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Cloud Netral\n",
        "if not data_netral.empty:\n",
        "    all_text_s2 = ' '.join(word for word in data_netral[\"full_text\"].dropna())\n",
        "    if all_text_s2:\n",
        "        wordcloud = WordCloud(colormap='Greens', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text_s2)\n",
        "        plt.figure(figsize=(9, 6))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title(\"Visualisasi Kata Netral\")\n",
        "        plt.margins(x=0, y=0)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "YZHwdzMnaTOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pembagian Data untuk Pelatihan\n",
        "print(\"\\nPembagian Data untuk Pelatihan\")\n",
        "x = df.full_text\n",
        "y = df.sentimen"
      ],
      "metadata": {
        "id": "GoFKDRDDH1-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n"
      ],
      "metadata": {
        "id": "IzzELb94ITgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nBanyak data x_train :',len(x_train))\n",
        "print('Banyak data x_test  :',len(x_test))\n",
        "print('Banyak data y_train :',len(y_train))\n",
        "print('Banyak data y_test  :',len(y_test))"
      ],
      "metadata": {
        "id": "F36rPMWWO5ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nMENAMPILKAN DATA TEST DAN TRAIN\")\n",
        "print(\"\\ndata pertama dari x_train (teks yang sudah diproses):\")\n",
        "print(x_train.head().to_string())\n",
        "print(\"\\ndata pertama dari x_test (teks yang sudah diproses):\")\n",
        "print(x_test.head().to_string())\n",
        "print(\"\\ndata pertama dari y_train (label sentimen):\")\n",
        "print(y_train.head())\n",
        "print(\"\\ndata pertama dari y_test (label sentimen):\")\n",
        "print(y_test.head())"
      ],
      "metadata": {
        "id": "Xk2wZ5fG9rwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nUkuran x_train: {x_train.shape}\")\n",
        "print(f\"Ukuran x_test: {x_test.shape}\")\n",
        "print(f\"Ukuran y_train: {y_train.shape}\")\n",
        "print(f\"Ukuran y_test: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "axVFzgBHtJJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pelatihan dan Evaluasi Model Naive Bayes dengan Hyperparameter Tuning\n",
        "print(\"pelatihan dan Evaluasi Model Naive Bayes\")\n",
        "# Definisi Pipeline untuk Naive Bayes\n",
        "pipeline_nb = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer()),\n",
        "    ('classifier', MultinomialNB())\n",
        "])"
      ],
      "metadata": {
        "id": "3nKMUWDGtNeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter Grid untuk GridSearchCV\n",
        "# Eksperimen dengan ngram_range dan max_features untuk TfidfVectorizer\n",
        "# Eksperimen dengan alpha untuk MultinomialNB\n",
        "param_grid = {\n",
        "    'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)], # Coba unigram, bigram, trigram\n",
        "    'vectorizer__max_features': [5000, 10000, 15000], # Jumlah fitur TF-IDF\n",
        "    'classifier__alpha': [0.01, 0.1, 0.5, 1.0, 2.0] # Smoothing parameter untuk Naive Bayes\n",
        "}"
      ],
      "metadata": {
        "id": "ewckpQaatmWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi GridSearchCV\n",
        "# cv=5 berarti 5-fold cross-validation\n",
        "# n_jobs=-1 berarti menggunakan semua core CPU yang tersedia\n",
        "grid_search_nb = GridSearchCV(pipeline_nb, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "print(\"\\nMemulai GridSearchCV untuk Naive Bayes (ini mungkin memakan waktu)...\")\n",
        "grid_search_nb.fit(x_train, y_train)\n",
        "print(\"\\nGridSearchCV selesai.\")\n",
        "print(f\"Best parameters found: {grid_search_nb.best_params_}\")\n",
        "print(f\"Best cross-validation accuracy: {grid_search_nb.best_score_:.4f}\")"
      ],
      "metadata": {
        "id": "Cl6hgOlftsdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\ndata pertama dari x_train (teks yang sudah diproses):\")\n",
        "print(x_train.head().to_string())"
      ],
      "metadata": {
        "id": "qHUNe0ge9tp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\ndata pertama dari x_test (teks yang sudah diproses):\")\n",
        "print(x_test.head().to_string())"
      ],
      "metadata": {
        "id": "1nj8Mczo-HQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\ndata pertama dari y_train (label sentimen):\")\n",
        "print(y_train.head())"
      ],
      "metadata": {
        "id": "zA34MIrc-PZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\ndata pertama dari y_test (label sentimen):\")\n",
        "print(y_test.head())"
      ],
      "metadata": {
        "id": "RPvntRga-VS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nUkuran x_train: {x_train.shape}\")\n",
        "print(f\"Ukuran x_test: {x_test.shape}\")\n",
        "print(f\"Ukuran y_train: {y_train.shape}\")\n",
        "print(f\"Ukuran y_test: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "hfXusE_u-aoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pelatihan Model Naive Bayes dengan TF-IDF\n",
        "print(\"\\nPelatihan Model Naive Bayes dengan TF-IDF\")"
      ],
      "metadata": {
        "id": "4Ai0jPrO-f5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_features=5170)\n",
        "naive_bayes_classifier = MultinomialNB()\n",
        "model_naive_bayes = Pipeline([\n",
        "    ('vectorizer', tfidf_vectorizer),\n",
        "    ('classifier', naive_bayes_classifier)\n",
        "])"
      ],
      "metadata": {
        "id": "HgA4a63M-rYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Latih model Naive Bayes\n",
        "model_naive_bayes.fit(x_train, y_train)\n",
        "print(\"Model Naive Bayes berhasil dilatih.\")"
      ],
      "metadata": {
        "id": "2wJGiotaAnLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nMENAMPILKAN REPRESENTASI TF-IDF DAN PROBABILITAS\")\n",
        "trained_tfidf_vectorizer = model_naive_bayes.named_steps['vectorizer']\n",
        "X_train_tfidf_transformed = trained_tfidf_vectorizer.transform(x_train)\n",
        "X_test_tfidf_transformed = trained_tfidf_vectorizer.transform(x_test)\n",
        "print(f\"\\nShape TF-IDF dari x_train: {X_train_tfidf_transformed.shape}\")\n",
        "print(f\"Shape TF-IDF dari x_test: {X_test_tfidf_transformed.shape}\")\n"
      ],
      "metadata": {
        "id": "ps3lV2NwApQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nbaris pertama dari matriks TF-IDF x_train (sparse format):\")\n",
        "print(X_train_tfidf_transformed[:10])"
      ],
      "metadata": {
        "id": "mvVyOjQ5BNgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nJumlah fitur (kata unik) yang dipelajari oleh TF-IDF Vectorizer:\", len(trained_tfidf_vectorizer.get_feature_names_out()))\n",
        "print(\"\\nBeberapa fitur (kata) pertama yang dipelajari oleh TF-IDF Vectorizer:\")\n",
        "print(trained_tfidf_vectorizer.get_feature_names_out()[:20])"
      ],
      "metadata": {
        "id": "hoBSOzQ7BTwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Untuk melihat TF-IDF dari sebuah teks\n",
        "if not x_train.empty:\n",
        "    sample_text_train = x_train.iloc[0]\n",
        "    sample_tfidf_train = trained_tfidf_vectorizer.transform([sample_text_train])\n",
        "    print(f\"\\nTF-IDF untuk contoh teks pertama dari x_train ('{sample_text_train[:50]}...'):\")\n",
        "\n",
        "    print(sample_tfidf_train.toarray())\n",
        "\n",
        "    feature_names = trained_tfidf_vectorizer.get_feature_names_out()\n",
        "    print(\"Fitur dengan bobot non-nol untuk contoh teks ini:\")\n",
        "\n",
        "    non_zero_cols = sample_tfidf_train.nonzero()[1]\n",
        "    for col_idx in non_zero_cols:\n",
        "        print(f\"  {feature_names[col_idx]}: {sample_tfidf_train[0, col_idx]:.4f}\")\n"
      ],
      "metadata": {
        "id": "vBMHCoQ5Bcty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Probabilitas Prediksi\")\n",
        "y_proba_nb = model_naive_bayes.predict_proba(x_test)\n",
        "print(\"\\nProbabilitas prediksi untuk 10 sampel pertama dari x_test:\")\n",
        "proba_df = pd.DataFrame(y_proba_nb[:5], columns=model_naive_bayes.classes_)\n",
        "print(proba_df)"
      ],
      "metadata": {
        "id": "PfH3k-8zBxmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nContoh probabilitas untuk satu teks dari x_test:\")\n",
        "if not x_test.empty:\n",
        "    sample_text_test = x_test.iloc[0]\n",
        "    sample_proba = model_naive_bayes.predict_proba([sample_text_test])\n",
        "    print(f\"Teks: '{sample_text_test[:50]}...'\")\n",
        "    print(f\"Probabilitas: {sample_proba[0]}\")\n",
        "    # Menampilkan probabilitas dengan label kelas\n",
        "    for i, class_label in enumerate(model_naive_bayes.classes_):\n",
        "        print(f\"  {class_label}: {sample_proba[0][i]:.4f}\")\n",
        "    print(f\"Prediksi kelas: {model_naive_bayes.predict([sample_text_test])[0]}\")"
      ],
      "metadata": {
        "id": "qSntB7pmCJo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MENAMPILKAN TF-IDF DAN PROBABILITAS\n",
        "#print(\"\\nEvaluasi Model Naive Bayes\")\n",
        "y_pred_nb = model_naive_bayes.predict(x_test)"
      ],
      "metadata": {
        "id": "Vn56XrUkCQOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "cm_nb = confusion_matrix(y_test, y_pred_nb)\n",
        "disp_nb = ConfusionMatrixDisplay(confusion_matrix=cm_nb, display_labels=model_naive_bayes.classes_)\n",
        "disp_nb.plot(cmap='Blues')\n",
        "plt.title('Confusion Matrix Naive Bayes')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lRksOflsCjXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report\n",
        "matrix_nb = classification_report(y_test, y_pred_nb)\n",
        "print('Classification Report (Naive Bayes):\\n', matrix_nb)"
      ],
      "metadata": {
        "id": "DnYRIrx1Cuc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Informasi Internal Model Naive Bayes\n",
        "print(\"\\nINFORMASI INTERNAL MODEL NAIVE BAYES\")\n",
        "trained_vectorizer = model_naive_bayes.named_steps['vectorizer']\n",
        "trained_classifier = model_naive_bayes.named_steps['classifier']"
      ],
      "metadata": {
        "id": "4vSs_NFIDuPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = trained_vectorizer.vocabulary_\n",
        "print(\"\\nUkuran Vocabulary:\", len(vocabulary))"
      ],
      "metadata": {
        "id": "id9_MBbtD0-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tfidf_transformed_for_info = trained_vectorizer.transform(x_train) # Re-transform for info if needed\n",
        "print(f\"\\nShape Matriks TF-IDF (x_train): {X_train_tfidf_transformed_for_info.shape}\")"
      ],
      "metadata": {
        "id": "Xx6CuprXD31d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_labels = trained_classifier.classes_\n",
        "print(\"\\nNama Kelas (Sentimen):\", class_labels)\n"
      ],
      "metadata": {
        "id": "aYIQfRyMD7h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_prior_classes = trained_classifier.class_log_prior_\n",
        "print(\"\\nLog Probabilitas Prior Kelas:\")\n",
        "for i, label in enumerate(class_labels):\n",
        "    print(f\"  Log P({label}): {log_prior_classes[i]:.4f}\")"
      ],
      "metadata": {
        "id": "C6S9axlIECAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_likelihood_features = trained_classifier.feature_log_prob_\n",
        "print(f\"\\nShape Log Probabilitas Likelihood Fitur: {log_likelihood_features.shape}\")"
      ],
      "metadata": {
        "id": "Nfe4Odl1ErVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n10 Kata Teratas dengan Probabilitas Tertinggi per Kelas:\")\n",
        "feature_names = trained_vectorizer.get_feature_names_out()\n",
        "for i, class_label in enumerate(class_labels):\n",
        "    top_10_indices = log_likelihood_features[i, :].argsort()[-10:][::-1]\n",
        "    top_10_words = [feature_names[idx] for idx in top_10_indices]\n",
        "    top_10_probs = [f\"{log_likelihood_features[i, idx]:.4f}\" for idx in top_10_indices]\n",
        "    print(f\"  Kelas '{class_label}':\")\n",
        "    for j in range(10):\n",
        "        print(f\"    - {top_10_words[j]} (Log Prob: {top_10_probs[j]})\")"
      ],
      "metadata": {
        "id": "073K8kgjEvEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pengujian Model Naïve Bayes\n",
        "print(\"\\nPengujian Model Naive Bayes\")\n",
        "def classify_text_with_naive_bayes(input_text):\n",
        "\n",
        "    processed_text = input_text.lower()\n",
        "    processed_text = clean_twitter_text(processed_text)\n",
        "    processed_text = normalisasi(processed_text)\n",
        "    processed_text = stopword(processed_text) # Corrected function name\n",
        "    processed_text = stemming_text(processed_text) # Pastikan ini mengembalikan string\n",
        "    # Prediksi menggunakan pipeline Naive Bayes\n",
        "    prediction = model_naive_bayes.predict([processed_text])\n",
        "    # Probabilitas prediksi\n",
        "    prediction_proba = model_naive_bayes.predict_proba([processed_text])\n",
        "    return prediction[0], prediction_proba[0]\n",
        "\n",
        "input_text_user = input(\"\\nMasukkan teks yang ingin diklasifikasikan oleh Naive Bayes: \")\n",
        "results_nb_class, results_nb_proba = classify_text_with_naive_bayes(input_text_user)\n",
        "print(\"Input teks :\", input_text_user)\n",
        "print(f\"Hasil Klasifikasi (Multinomial Naive Bayes): {results_nb_class}\")\n",
        "print(\"Probabilitas untuk setiap kelas:\")\n",
        "for i, class_label in enumerate(model_naive_bayes.classes_):\n",
        "    print(f\"  {class_label}: {results_nb_proba[i]:.4f}\")"
      ],
      "metadata": {
        "id": "voOX6PttIrzL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}