{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiara070403/skripsi/blob/main/skripsifiks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMUmyHLxTPXg",
        "outputId": "37d1b58a-ed56-45e7-fa5c-5a28e35ff4e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m204.8/209.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from wordcloud) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud) (11.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from wordcloud) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "!pip install Sastrawi\n",
        "!pip install tweet-preprocessor\n",
        "!pip install textblob\n",
        "!pip install wordcloud\n",
        "!pip install nltk\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score, precision_score, f1_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import Sastrawi\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from transformers import pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/crawling.csv', index_col=0)\n",
        "df"
      ],
      "metadata": {
        "id": "lRjq11UeVWxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['created_at', 'full_text']]"
      ],
      "metadata": {
        "id": "F9GG4pj1VmkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "0QGG0mTHVuD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['full_text'] = df['full_text'].astype(str).str.lower()  # Mengubah teks menjadi lowercase\n",
        "df['full_text'] = df['full_text'].replace('false', pd.NA)  # Mengganti 'false' dengan pd.NA (Not Available)\n",
        "df = df.dropna(subset=['full_text'])"
      ],
      "metadata": {
        "id": "x7E237CWFhi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop_duplicates(subset=['full_text'])"
      ],
      "metadata": {
        "id": "IllVA0F5Vwqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "5WeD8UkzV1qH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "-qrKOGdaV2hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "qJlOWWq3V9Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "V6dTymLsWBoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Case Folding\n",
        "df['full_text'] = df['full_text'].str.lower()\n",
        "df"
      ],
      "metadata": {
        "id": "9G2F9u6rWCtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning\n",
        "def clean_twitter_text(text):\n",
        "  text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
        "  text = re.sub(r'#\\w+', '', text)\n",
        "  text = re.sub(r'RT[\\s]+', '', text)\n",
        "  text = re.sub(r'https?://\\S+', '', text)\n",
        "\n",
        "  text = re.sub(r'[^A-Za-z0-9 ]', '', text)\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "  return text\n",
        "\n",
        "df['full_text'] = df['full_text'].apply(clean_twitter_text)"
      ],
      "metadata": {
        "id": "yPyxz9vjWFt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "XwmkQ_juGAZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_tokens_by_length(df, column, min_words, max_words):\n",
        "\n",
        "    words_count = df[column].astype(str).apply(lambda x: len(x.split()))\n",
        "\n",
        "    mask = (words_count >= min_words) & (words_count <= max_words)\n",
        "\n",
        "    filtered_df = df[mask]\n",
        "    return filtered_df\n",
        "\n",
        "min_words = 3\n",
        "max_words = 4000\n",
        "df = filter_tokens_by_length(df, 'full_text', min_words, max_words)"
      ],
      "metadata": {
        "id": "bnbK-nBzFv7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalisasi Data\n",
        "norm = {'kekekekegiatanananan':'kegiatan', 'press release': 'siaran pers', 'ukamu': 'unit kegiatan mahasiswa', 'giat': 'kegiatan', 'luring': 'luring', 'sekre': 'sekretariat', 'mapala': 'mahasiswa pencinta alam', 'kocak': 'lucu', 'enak': 'enak', 'bilang': 'bilang', 'sowan': 'mengunjungi', 'buka': 'membuka', 'sampai': 'hingga', 'pukul': 'jam', 'sd': 'sampai', 'kapan': 'kapan', 'mau': 'ingin', 'join': 'bergabung', 'sama': 'bersama', 'bisa': 'dapat', 'bantu': 'membantu', 'kamu': 'kamu', 'sama': 'bersama', 'bisa': 'dapat', 'sama': 'bersama', ' mikir2 ':' mikir ', 'perkoempoelan':'perkumpulan', 'sowan':'menghadap', 'genjot':'menyerang', 'wkwkw':'hahaha', 'gtgtgt':'.', 'th':'tahun', 'ukamu':'ukm', 'bejat':'rusak', 'pentjinta':'pencinta', 'pecinta':'pencinta', ' siaaapp ':' siap ', 'okaaay ':'oke ', 'udh ':'sudah ','ga ':'tidak ',' gaskeun':' ayo ', 'wowww ':'wow ', ' haaayyuukkk ':' ayo ', ' yg ':' yang ', ' udh ':' udah ', 'wkwk ':' ', ' min ':' kak ', ' malem ':' malam', ' malem2 ':' malam ', ' sm ':' sama ', ' dy ':' dia ', ' lg ':' lagi ', ' skrg ':' sekarang ', ' ddpn ':' didepan ', ' makasi ':' makasih ', ' pertamaz ':' pertamax ', ' jg ':' juga ', ' donk ':' dong ', ' ikutann ':' ikutan ', ' banyakk ':' banyak ', ' twt ':' tweet', 'mantaap ':'mantap ', ' juarak':' juara ', 'daridulu ':'dari dulu ', 'siapp ':'siap ', ' gamau ':' tidak mau ', ' sll ':' selalu ', ' qu ':' aku ', ' krn ':' karena ', ' irii':' iri', ' muluu ':' terus ', 'mada ':'masa ', 'jgn ':'jangan ', ' jgn ':' jangan ', ' muluuu ':' terus ', 'ntar ':'nanti ', ' awtnya':' awetnya', 'gg ':'keren ', ' kerennn':' keren ', ' bisaa ':' bisa ', 'gaaa':'tidak ', \" yg \": \" yang \", ' nyampe':' sampai', ' nyampe ':' sampai ', ' lu ':' kamu ', ' ikhlaaasss ':' ikhlas ', ' gak ':' tidak ', ' klo ':' kalo ', ' amp ': ' sampai ', ' ga ':' tidak ', ' yaaaa':' ya ', 'betolll ':'betul ', ' kaga ':' tidak ', ' idk ':' tidak tahu ', ' jkt ':' jakarta ', ' lo ':' kamu ', ' bjir ':' ', ' kek ':' seperti ', ' yg ':' yang ', ' utk ':' untuk ', 'kismin ':'miskin ', ' kismin ':' miskin ', ' pd ':' pada ', ' dgn ':' dengan ', ' ituu ':' itu ', ' jg ':' juga ', 'yoi':'iya ', ' yoi ':' iya ', 'org2 ':'orang ', ' tak ':' tidak ', ' kyk ':' seperti ', ' sbg ':' sebagai ', ' anjjjj ':' ', ' bgt ':' banget ', 'km ':'kamu ', ' km ':' kamu', ' byk ':' banyak ', ' lg ':' lagi ', ' mrk ':' mereka ', ' blm ':' belum '}\n",
        "\n",
        "def normalisasi(str_text):\n",
        "  for i in norm:\n",
        "    str_text = str_text.replace(i, norm[i])\n",
        "  return str_text\n",
        "\n",
        "df['full_text'] = df['full_text'].apply(lambda x: normalisasi(x))\n",
        "df"
      ],
      "metadata": {
        "id": "D1xqPx6sWXpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords Removal\n",
        "import Sastrawi\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
        "more_stop_words = ['anjayyy']\n",
        "\n",
        "stop_words = StopWordRemoverFactory().get_stop_words()\n",
        "stop_words.extend(more_stop_words)\n",
        "\n",
        "new_array = ArrayDictionary(stop_words)\n",
        "stop_words_remover_new = StopWordRemover(new_array)\n",
        "\n",
        "def stopword(str_text):\n",
        "  str_text = stop_words_remover_new.remove(str_text)\n",
        "  return str_text\n",
        "\n",
        "df['full_text'] = df['full_text'].apply(lambda x: stopword(x))\n",
        "df"
      ],
      "metadata": {
        "id": "YrX60CN_WZJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenisasi\n",
        "tokenized = df['full_text'].apply(lambda x:x.split())\n",
        "df"
      ],
      "metadata": {
        "id": "GYHT_zaiWekG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "def stemming(full_text):\n",
        "  factory = StemmerFactory()\n",
        "  stemmer = factory.create_stemmer()\n",
        "  do = []\n",
        "  for w in full_text:\n",
        "    dt = stemmer.stem(w)\n",
        "    do.append(dt)\n",
        "  d_clean = []\n",
        "  d_clean = \" \".join(do)\n",
        "  print(d_clean)\n",
        "  return d_clean\n",
        "\n",
        "tokenized = tokenized.apply(stemming)\n",
        "data_clean = pd.read_csv('/content/preprocessing.csv', encoding='latin1')\n"
      ],
      "metadata": {
        "id": "e7rqJbrkWkZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/preprocessing.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "B4UoQgrVV7N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy = df.copy()"
      ],
      "metadata": {
        "id": "HxA9Rkl0orzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Labeling\n",
        "sentimen = []\n",
        "for index, row in df_copy.iterrows():\n",
        "  if row[\"full_text\"] == 1 or row[\"full_text\"] == 2:\n",
        "    sentimen.append(0)\n",
        "  else :\n",
        "    sentimen.append(1)"
      ],
      "metadata": {
        "id": "HatvHGsCppmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy[\"sentimen\"] = sentimen\n",
        "df"
      ],
      "metadata": {
        "id": "AY-suuxdp3vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.head()"
      ],
      "metadata": {
        "id": "I_wlgVn9p7oY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentimen_counts = df.sentimen.value_counts()\n",
        "sentimen_counts"
      ],
      "metadata": {
        "id": "NC3qTcjIqDjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "2px2lkJVqN83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggabungkan kedua attribut\n",
        "at1 = pd.read_csv('/content/labelingg.csv')\n",
        "at2 = pd.read_csv('/content/crawling.csv')\n",
        "att2 = at1['sentimen']\n",
        "\n",
        "result = pd.concat([at1, att2], axis=1)"
      ],
      "metadata": {
        "id": "3fapCM-4LF8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.head()"
      ],
      "metadata": {
        "id": "iYK-21bpLoJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung Kata Dengan TF-IDF\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
      ],
      "metadata": {
        "id": "-eA3BltTMnIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text = result['full_text']"
      ],
      "metadata": {
        "id": "qpZrji6YMyr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text.isnull().sum()"
      ],
      "metadata": {
        "id": "Q04ULpfXM_Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text = full_text.fillna('tidak ada komentar')"
      ],
      "metadata": {
        "id": "D53wbqxVNDi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer()\n",
        "term_fit = cv.fit(full_text)\n",
        "\n",
        "print(len(term_fit.vocabulary_))"
      ],
      "metadata": {
        "id": "3Mc0ZxCVNKe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "term_fit.vocabulary_"
      ],
      "metadata": {
        "id": "dlUF6BRTNRBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "term_frequency_all = term_fit.transform(full_text)\n",
        "print(term_frequency_all)"
      ],
      "metadata": {
        "id": "h88kRd4ZNc8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text_tf = full_text[1]\n",
        "print(full_text_tf)"
      ],
      "metadata": {
        "id": "N1G-RmMLNvSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "term_frequency = term_fit.transform([full_text_tf])\n",
        "print(term_frequency)"
      ],
      "metadata": {
        "id": "z2AVbC3UN54j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dokumen = term_fit.transform(full_text)\n",
        "tfidf_transformer = TfidfTransformer().fit(dokumen)\n",
        "print(tfidf_transformer.idf_)\n",
        "\n",
        "tfidf = tfidf_transformer.transform(term_frequency)\n",
        "print(tfidf)"
      ],
      "metadata": {
        "id": "Ws_SjteeOBIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_s0 = df_copy[df_copy[\"sentimen\"] == 0]"
      ],
      "metadata": {
        "id": "sL26NWLnqnuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_s0[\"full_text\"] = train_s0[\"full_text\"].fillna(\"tidak ada komentar\")"
      ],
      "metadata": {
        "id": "-gtI1vxYONud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "Kb9NpWVNrAnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_s0.head()"
      ],
      "metadata": {
        "id": "Bgo-1FAqqtZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "g6nt8JO3r5Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisasi\n",
        "sns.set_palette(\"pastel\")\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='sentimen', data=df)\n",
        "plt.title('Distribusi Sentimen')\n",
        "plt.xlabel('Sentimen')\n",
        "plt.ylabel('Jumlah')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z-PhITyraCV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_negatif = df[df['sentimen'] == 'negative']\n",
        "data_positif = df[df['sentimen'] == 'positive']\n",
        "data_netral = df[df['sentimen'] == 'neutral']"
      ],
      "metadata": {
        "id": "pijkEFyJaIGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_text_s1 = ' '.join(word for word in data_positif[\"full_text\"])\n",
        "wordcloud = WordCloud(colormap='Blues', width=1000, height=1000, mode=\"RGBA\", background_color='white').generate(all_text_s1)\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Visualisasi Kata Positif\")\n",
        "plt.margins(x=0, y=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sYGzjYkbaMOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_text_s0 = ' '.join(word for word in data_negatif[\"full_text\"])\n",
        "wordcloud = WordCloud(colormap='Reds', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text_s0)\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Visualisasi Kata Negatif\")\n",
        "plt.margins(x=0, y=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Btj3O5cYaP1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_text_s2 = ' '.join(word for word in data_netral[\"full_text\"])\n",
        "wordcloud = WordCloud(colormap='Greens', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text_s2)\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Visualisasi Kata Netral\")\n",
        "plt.margins(x=0, y=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YZHwdzMnaTOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Data (TF-IDF)\n",
        "result['full_text'] = result['full_text'].fillna(\"Tidak ada komentar\")"
      ],
      "metadata": {
        "id": "tjaqSEyFPcvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(result['full_text'], result['sentimen'],\n",
        "                                                    test_size=0.1, stratify=result['sentimen'], random_state=30)"
      ],
      "metadata": {
        "id": "hzyZ9URwPncy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "sWMtN8T7Pw9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(decode_error='replace', encoding='utf-8')"
      ],
      "metadata": {
        "id": "9SNDjJkKP0iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "CowSN-uhQR2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "0ZpNa2KRP6R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.toarray()"
      ],
      "metadata": {
        "id": "ARtc0B76P9DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X_test.toarray()"
      ],
      "metadata": {
        "id": "vL0ueXFKsTFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes Classifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb = GaussianNB()"
      ],
      "metadata": {
        "id": "AWDYFspJsZnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "cv_method = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=999)\n",
        "\n",
        "params_NB = {'var_smoothing': np.logspace(0, -9, num=100)}\n",
        "gscv_nb = GridSearchCV(estimator=nb,\n",
        "                        param_grid=params_NB,\n",
        "                        cv = cv_method,\n",
        "                        verbose = 1,\n",
        "                        scoring = 'accuracy')\n",
        "\n",
        "gscv_nb.fit(X_train, y_train)\n",
        "gscv_nb.best_params_"
      ],
      "metadata": {
        "id": "p8EPwEtZsfXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=df.full_text\n",
        "y=df.sentimen"
      ],
      "metadata": {
        "id": "qSnjugrkaYHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "3HGlWtVaahER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Banyak data x_train :',len(x_train))\n",
        "print('Banyak data x_test  :',len(x_test))\n",
        "print('Banyak data y_train :',len(y_train))\n",
        "print('Banyak data y_test  :',len(y_test))"
      ],
      "metadata": {
        "id": "xoQRN5HdakPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING DATA\n",
        "cvec=CountVectorizer()\n",
        "tvec=TfidfVectorizer()\n",
        "hvec=HashingVectorizer()"
      ],
      "metadata": {
        "id": "i1nqRS7raunM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "clf1 = MultinomialNB()"
      ],
      "metadata": {
        "id": "yngXfMGfavte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = Pipeline([('vectorizer',tvec)\n",
        "                 ,('classifier',clf1)])"
      ],
      "metadata": {
        "id": "SY0ZQJe2ax_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "j4w9LBh1a1tQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hasil1=model1.predict(x_test)"
      ],
      "metadata": {
        "id": "P1qOA0fLa5Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = classification_report(y_test,hasil1)\n",
        "print('Classification report : \\n',matrix)"
      ],
      "metadata": {
        "id": "uj53mC1ba8Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTING\n",
        "def classify_text(input_text):\n",
        "    models = [\n",
        "        ('MultinomialNB', model_multinomialNaiveBayes)\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models:\n",
        "        prediction = model.predict([input_text])\n",
        "        results[name] = prediction[0]\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "gU_J8wzTbAKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_multinomialNaiveBayes = model1.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "BFHbBZ91bSpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = input(\"Masukkan teks yang ingin diklasifikasikan: \")\n",
        "\n",
        "results = classify_text(input_text)\n",
        "\n",
        "print(\"Input kata :\", input_text)\n",
        "\n",
        "print(\"\\nHasil Klasifikasi:\")\n",
        "for model, prediction in results.items():\n",
        "    print(f\"{model}: {prediction}\")"
      ],
      "metadata": {
        "id": "XTZMPGIqbXv5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}