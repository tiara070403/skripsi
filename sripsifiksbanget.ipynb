{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiara070403/skripsi/blob/main/sripsifiksbanget.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMUmyHLxTPXg",
        "outputId": "56ff3f68-a337-4d08-9cb2-654754968988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m153.6/209.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from wordcloud) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud) (11.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from wordcloud) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "!pip install Sastrawi\n",
        "!pip install tweet-preprocessor\n",
        "!pip install textblob\n",
        "!pip install wordcloud\n",
        "!pip install nltk\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score, precision_score, f1_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import Sastrawi\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from transformers import pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/crawling.csv', index_col=0)"
      ],
      "metadata": {
        "id": "lRjq11UeVWxi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy = df.copy()"
      ],
      "metadata": {
        "id": "V1FHaiZNZbX2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['created_at', 'full_text']]"
      ],
      "metadata": {
        "id": "F9GG4pj1VmkP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "0QGG0mTHVuD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "824b4b24-2147-499f-856b-98542392c303"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 5349 entries, 1858070162362204643 to 1230765438897815552\n",
            "Data columns (total 2 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   created_at  5349 non-null   object\n",
            " 1   full_text   5349 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 125.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['full_text'] = df['full_text'].astype(str).str.lower()  # Mengubah teks menjadi lowercase\n",
        "df['full_text'] = df['full_text'].replace('false', pd.NA)\n",
        "df = df.dropna(subset=['full_text'])"
      ],
      "metadata": {
        "id": "x7E237CWFhi2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop_duplicates(subset=['full_text'])"
      ],
      "metadata": {
        "id": "IllVA0F5Vwqh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "5WeD8UkzV1qH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eaae6c1-685c-4027-d8d6-a23e8eb7c479"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(0)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nJumlah duplikat setelah drop: {df.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "5JSLtJJcySE_",
        "outputId": "a69f453c-cf60-483d-a3fc-67c0384a0660",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Jumlah duplikat setelah drop: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "-qrKOGdaV2hs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nJumlah nilai null setelah dropna: \\n{df.isnull().sum()}\")"
      ],
      "metadata": {
        "id": "kR8Emam2yVom",
        "outputId": "545ccb92-aae6-4561-85a5-1d8b762a1176",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Jumlah nilai null setelah dropna: \n",
            "created_at    0\n",
            "full_text     0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "V6dTymLsWBoO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "9c75268f-7127-4b85-c0ee-e0dca66b7fe7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "created_at    0\n",
              "full_text     0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>created_at</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>full_text</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Case Folding\n",
        "df['full_text'] = df['full_text'].str.lower()\n",
        "print(\"\\nDataFrame setelah Case Folding:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "9G2F9u6rWCtX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2f69a5e-205b-4f87-ec7d-135a7402c5af"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame setelah Case Folding:\n",
            "                                         created_at  \\\n",
            "conversation_id_str                                   \n",
            "1858070162362204643  Sun Nov 17 08:50:22 +0000 2024   \n",
            "1846529554657239219  Wed Oct 16 12:32:07 +0000 2024   \n",
            "1467691301155979267  Wed Oct 09 13:58:44 +0000 2024   \n",
            "1836014724523458593  Tue Sep 17 12:09:56 +0000 2024   \n",
            "1835164951125917967  Sun Sep 15 03:53:14 +0000 2024   \n",
            "\n",
            "                                                             full_text  \n",
            "conversation_id_str                                                     \n",
            "1858070162362204643                lucu banget mahasiswa pencinta alam  \n",
            "1846529554657239219  rusak mahasiswa pencinta alam jambi menyerang ...  \n",
            "1467691301155979267  perkumpulan pencinta alam lahir tahun 1953 did...  \n",
            "1836014724523458593  siaran pers menghadap ukamu ukamu unit pandu l...  \n",
            "1835164951125917967  siaran pers menghadap ukamu ukamu unit pandu l...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-22-2702663551.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['full_text'] = df['full_text'].str.lower()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning\n",
        "def clean_twitter_text(text):\n",
        "  text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
        "  text = re.sub(r'#\\w+', '', text)\n",
        "  text = re.sub(r'RT[\\s]+', '', text)\n",
        "  text = re.sub(r'https?://\\S+', '', text)\n",
        "  text = re.sub(r'[^A-Za-z0-9 ]', '', text)\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "  return text\n",
        "df['full_text'] = df['full_text'].apply(clean_twitter_text)\n",
        "print(f\"\\nShape setelah Cleaning: {df.shape}\")\n",
        "print(\"DataFrame setelah Cleaning:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "yPyxz9vjWFt-",
        "outputId": "013f032e-c976-4a02-ce11-6b1c76813de7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shape setelah Cleaning: (5170, 2)\n",
            "DataFrame setelah Cleaning:\n",
            "                                         created_at  \\\n",
            "conversation_id_str                                   \n",
            "1858070162362204643  Sun Nov 17 08:50:22 +0000 2024   \n",
            "1846529554657239219  Wed Oct 16 12:32:07 +0000 2024   \n",
            "1467691301155979267  Wed Oct 09 13:58:44 +0000 2024   \n",
            "1836014724523458593  Tue Sep 17 12:09:56 +0000 2024   \n",
            "1835164951125917967  Sun Sep 15 03:53:14 +0000 2024   \n",
            "\n",
            "                                                             full_text  \n",
            "conversation_id_str                                                     \n",
            "1858070162362204643                lucu banget mahasiswa pencinta alam  \n",
            "1846529554657239219  rusak mahasiswa pencinta alam jambi menyerang ...  \n",
            "1467691301155979267  perkumpulan pencinta alam lahir tahun 1953 did...  \n",
            "1836014724523458593  siaran pers menghadap ukamu ukamu unit pandu l...  \n",
            "1835164951125917967  siaran pers menghadap ukamu ukamu unit pandu l...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-23-2312499976.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['full_text'] = df['full_text'].apply(clean_twitter_text)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_tokens_by_length(df, column, min_words, max_words):\n",
        "    words_count = df[column].astype(str).apply(lambda x: len(x.split()))\n",
        "    mask = (words_count >= min_words) & (words_count <= max_words)\n",
        "    filtered_df = df[mask]\n",
        "    return filtered_df\n",
        "min_words = 3\n",
        "max_words = 4000\n",
        "df = filter_tokens_by_length(df, 'full_text', min_words, max_words)\n",
        "print(f\"\\nShape setelah Filter Token Length: {df.shape}\")\n",
        "print(\"DataFrame setelah Filter Token Length:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "bnbK-nBzFv7G",
        "outputId": "1b03d6cb-0530-460f-e017-67648e651774",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shape setelah Filter Token Length: (5151, 2)\n",
            "DataFrame setelah Filter Token Length:\n",
            "                                         created_at  \\\n",
            "conversation_id_str                                   \n",
            "1858070162362204643  Sun Nov 17 08:50:22 +0000 2024   \n",
            "1846529554657239219  Wed Oct 16 12:32:07 +0000 2024   \n",
            "1467691301155979267  Wed Oct 09 13:58:44 +0000 2024   \n",
            "1836014724523458593  Tue Sep 17 12:09:56 +0000 2024   \n",
            "1835164951125917967  Sun Sep 15 03:53:14 +0000 2024   \n",
            "\n",
            "                                                             full_text  \n",
            "conversation_id_str                                                     \n",
            "1858070162362204643                lucu banget mahasiswa pencinta alam  \n",
            "1846529554657239219  rusak mahasiswa pencinta alam jambi menyerang ...  \n",
            "1467691301155979267  perkumpulan pencinta alam lahir tahun 1953 did...  \n",
            "1836014724523458593  siaran pers menghadap ukamu ukamu unit pandu l...  \n",
            "1835164951125917967  siaran pers menghadap ukamu ukamu unit pandu l...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalisasi Data\n",
        "norm = {'kekekekegiatanananan':'kegiatan', 'press release': 'siaran pers', 'ukamu': 'unit kegiatan mahasiswa', 'giat': 'kegiatan', 'luring': 'luring', 'sekre': 'sekretariat', 'mapala': 'mahasiswa pencinta alam', 'kocak': 'lucu', 'enak': 'enak', 'bilang': 'bilang', 'sowan': 'mengunjungi', 'buka': 'membuka', 'sampai': 'hingga', 'pukul': 'jam', 'sd': 'sampai', 'kapan': 'kapan', 'mau': 'ingin', 'join': 'bergabung', 'sama': 'bersama', 'bisa': 'dapat', 'bantu': 'membantu', 'kamu': 'kamu', 'sama': 'bersama', 'bisa': 'dapat', 'sama': 'bersama', ' mikir2 ':' mikir ', 'perkoempoelan':'perkumpulan', 'sowan':'menghadap', 'genjot':'menyerang', 'wkwkw':'hahaha', 'gtgtgt':'.', 'th':'tahun', 'ukamu':'ukm', 'bejat':'rusak', 'pentjinta':'pencinta', 'pecinta':'pencinta', ' siaaapp ':' siap ', 'okaaay ':'oke ', 'udh ':'sudah ','ga ':'tidak ',' gaskeun':' ayo ', 'wowww ':'wow ', ' haaayyuukkk ':' ayo ', ' yg ':' yang ', ' udh ':' udah ', 'wkwk ':' ', ' min ':' kak ', ' malem ':' malam', ' malem2 ':' malam ', ' sm ':' sama ', ' dy ':' dia ', ' lg ':' lagi ', ' skrg ':' sekarang ', ' ddpn ':' didepan ', ' makasi ':' makasih ', ' pertamaz ':' pertamax ', ' jg ':' juga ', ' donk ':' dong ', ' ikutann ':' ikutan ', ' banyakk ':' banyak ', ' twt ':' tweet', 'mantaap ':'mantap ', ' juarak':' juara ', 'daridulu ':'dari dulu ', 'siapp ':'siap ', ' gamau ':' tidak mau ', ' sll ':' selalu ', ' qu ':' aku ', ' krn ':' karena ', ' irii':' iri', ' muluu ':' terus ', 'mada ':'masa ', 'jgn ':'jangan ', ' jgn ':' jangan ', ' muluuu ':' terus ', 'ntar ':'nanti ', ' awtnya':' awetnya', 'gg ':'keren ', ' kerennn':' keren ', ' bisaa ':' bisa ', 'gaaa':'tidak ', \" yg \": \" yang \", ' nyampe':' sampai', ' nyampe ':' sampai ', ' lu ':' kamu ', ' ikhlaaasss ':' ikhlas ', ' gak ':' tidak ', ' klo ':' kalo ', ' amp ': ' sampai ', ' ga ':' tidak ', ' yaaaa':' ya ', 'betolll ':'betul ', ' kaga ':' tidak ', ' idk ':' tidak tahu ', ' jkt ':' jakarta ', ' lo ':' kamu ', ' bjir ':' ', ' kek ':' seperti ', ' yg ':' yang ', ' utk ':' untuk ', 'kismin ':'miskin ', ' kismin ':' miskin ', ' pd ':' pada ', ' dgn ':' dengan ', ' ituu ':' itu ', ' jg ':' juga ', 'yoi':'iya ', ' yoi ':' iya ', 'org2 ':'orang ', ' tak ':' tidak ', ' kyk ':' seperti ', ' sbg ':' sebagai ', ' anjjjj ':' ', ' bgt ':' banget ', 'km ':'kamu ', ' km ':' kamu', ' byk ':' banyak ', ' lg ':' lagi ', ' mrk ':' mereka ', ' blm ':' belum '}\n",
        "def normalisasi(str_text):\n",
        "  for i in norm:\n",
        "    str_text = str_text.replace(i, norm[i])\n",
        "  return str_text\n",
        "df['full_text'] = df['full_text'].apply(lambda x: normalisasi(x))\n",
        "print(\"\\nDataFrame setelah Normalisasi:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "D1xqPx6sWXpD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f196e4-e887-4934-f840-7addb400e69b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame setelah Normalisasi:\n",
            "                                         created_at  \\\n",
            "conversation_id_str                                   \n",
            "1858070162362204643  Sun Nov 17 08:50:22 +0000 2024   \n",
            "1846529554657239219  Wed Oct 16 12:32:07 +0000 2024   \n",
            "1467691301155979267  Wed Oct 09 13:58:44 +0000 2024   \n",
            "1836014724523458593  Tue Sep 17 12:09:56 +0000 2024   \n",
            "1835164951125917967  Sun Sep 15 03:53:14 +0000 2024   \n",
            "\n",
            "                                                             full_text  \n",
            "conversation_id_str                                                     \n",
            "1858070162362204643                lucu banget mahasiswa pencinta alam  \n",
            "1846529554657239219  rusak mahasiswa pencinta alam jambi menyerang ...  \n",
            "1467691301155979267  perkumpulan pencinta alam lahir tahun 1953 did...  \n",
            "1836014724523458593  siaran pers menghadap ukamu ukamu unit pandu l...  \n",
            "1835164951125917967  siaran pers menghadap ukamu ukamu unit pandu l...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-26-2327349263.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['full_text'] = df['full_text'].apply(lambda x: normalisasi(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords Removal\n",
        "import Sastrawi\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
        "more_stop_words = ['anjayyy']\n",
        "stop_words = StopWordRemoverFactory().get_stop_words()\n",
        "stop_words.extend(more_stop_words)\n",
        "new_array = ArrayDictionary(stop_words)\n",
        "stop_words_remover_new = StopWordRemover(new_array)\n",
        "def stopword(str_text):\n",
        "  str_text = stop_words_remover_new.remove(str_text)\n",
        "  return str_text\n",
        "df['full_text'] = df['full_text'].apply(lambda x: stopword(x))\n",
        "print(\"\\nDataFrame setelah Stopwords Removal:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "YrX60CN_WZJk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "286c1c10-75b9-4f2c-ef75-0bea45b19184"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame setelah Stopwords Removal:\n",
            "                                         created_at  \\\n",
            "conversation_id_str                                   \n",
            "1858070162362204643  Sun Nov 17 08:50:22 +0000 2024   \n",
            "1846529554657239219  Wed Oct 16 12:32:07 +0000 2024   \n",
            "1467691301155979267  Wed Oct 09 13:58:44 +0000 2024   \n",
            "1836014724523458593  Tue Sep 17 12:09:56 +0000 2024   \n",
            "1835164951125917967  Sun Sep 15 03:53:14 +0000 2024   \n",
            "\n",
            "                                                             full_text  \n",
            "conversation_id_str                                                     \n",
            "1858070162362204643                lucu banget mahasiswa pencinta alam  \n",
            "1846529554657239219  rusak mahasiswa pencinta alam jambi menyerang ...  \n",
            "1467691301155979267  perkumpulan pencinta alam lahir tahun 1953 did...  \n",
            "1836014724523458593  siaran pers menghadap ukamu ukamu unit pandu l...  \n",
            "1835164951125917967  siaran pers menghadap ukamu ukamu unit pandu l...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-27-1207040705.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['full_text'] = df['full_text'].apply(lambda x: stopword(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenisasi\n",
        "tokenized_for_stemming = df['full_text'].apply(lambda x: x.split())\n",
        "print(\"\\nTokenisasi (sebelum Stemming):\")\n",
        "print(tokenized_for_stemming.head())"
      ],
      "metadata": {
        "id": "GYHT_zaiWekG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aae207eb-3aa6-496d-e91d-415e0200f2ea"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenisasi (sebelum Stemming):\n",
            "conversation_id_str\n",
            "1858070162362204643            [lucu, banget, mahasiswa, pencinta, alam]\n",
            "1846529554657239219    [rusak, mahasiswa, pencinta, alam, jambi, meny...\n",
            "1467691301155979267    [perkumpulan, pencinta, alam, lahir, tahun, 19...\n",
            "1836014724523458593    [siaran, pers, menghadap, ukamu, ukamu, unit, ...\n",
            "1835164951125917967    [siaran, pers, menghadap, ukamu, ukamu, unit, ...\n",
            "Name: full_text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "def stemming_text(list_of_tokens): # Mengubah fungsi agar menerima list token\n",
        "  factory = StemmerFactory()\n",
        "  stemmer = factory.create_stemmer()\n",
        "  stemmed_tokens = [stemmer.stem(w) for w in list_of_tokens]\n",
        "  return \" \".join(stemmed_tokens) # Mengembalikan string\n",
        "df['full_text'] = tokenized_for_stemming.apply(stemming_text) # Terapkan stemming pada list token\n",
        "tokenized.to_csv(\"/content/Stemming.csv\", index=False)\n",
        "print(\"\\nDataFrame setelah Stemming:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "mH3OG0pl4Afv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/preprocessing.csv')"
      ],
      "metadata": {
        "id": "B4UoQgrVV7N-",
        "outputId": "220a908c-542d-4add-a171-09fb5d5537c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/preprocessing.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-32-3034284026.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/preprocessing.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/preprocessing.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Labeling\n",
        "import pandas as pd\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"w11wo/indonesian-roberta-base-sentiment-classifier\")\n",
        "\n",
        "def prediksi_sentimen(teks):\n",
        "    hasil = classifier(teks)\n",
        "\n",
        "    return hasil[0]['label']\n",
        "\n",
        "df['sentimen'] = df['full_text'].apply(prediksi_sentimen)"
      ],
      "metadata": {
        "id": "Gb2EqsnfW_wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('/content/labeling2.csv', index=False)"
      ],
      "metadata": {
        "id": "apR-bYANuFPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentimen_counts = df.sentimen.value_counts()\n",
        "sentimen_counts"
      ],
      "metadata": {
        "id": "7bNw5vZp492w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "2px2lkJVqN83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisasi\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_palette(\"pastel\")\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='sentimen', data=df)\n",
        "plt.title('Analisis Sentimen Publik di X Terhadap Peran Mahasiswa Pencinta Alam dalam Pelestarian Lingkungan')\n",
        "plt.xlabel('Sentimen')\n",
        "plt.ylabel('Jumlah')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z-PhITyraCV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_negatif = df[df['sentimen'] == 'negative']\n",
        "data_positif = df[df['sentimen'] == 'positive']\n",
        "data_netral = df[df['sentimen'] == 'neutral']"
      ],
      "metadata": {
        "id": "xPHQR9p_Oeeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_text_s1 = ' '.join(word for word in data_positif[\"full_text\"])\n",
        "wordcloud = WordCloud(colormap='Blues', width=1000, height=1000, mode=\"RGBA\", background_color='white').generate(all_text_s1)\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Visualisasi Kata Positif\")\n",
        "plt.margins(x=0, y=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sYGzjYkbaMOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_text_s0 = ' '.join(word for word in data_negatif[\"full_text\"])\n",
        "wordcloud = WordCloud(colormap='Reds', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text_s0)\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Visualisasi Kata Negatif\")\n",
        "plt.margins(x=0, y=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Btj3O5cYaP1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_text_s2 = ' '.join(word for word in data_netral[\"full_text\"])\n",
        "wordcloud = WordCloud(colormap='Greens', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text_s2)\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Visualisasi Kata Netral\")\n",
        "plt.margins(x=0, y=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YZHwdzMnaTOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spliting Data\n",
        "x=df.full_text\n",
        "y=df.sentimen"
      ],
      "metadata": {
        "id": "GoFKDRDDH1-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "IzzELb94ITgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Banyak data x_train :',len(x_train))\n",
        "print('Banyak data x_test  :',len(x_test))\n",
        "print('Banyak data y_train :',len(y_train))\n",
        "print('Banyak data y_test  :',len(y_test))"
      ],
      "metadata": {
        "id": "F36rPMWWO5ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Data (Naive Bayes)\n",
        "cvec=CountVectorizer()\n",
        "tvec=TfidfVectorizer()\n",
        "hvec=HashingVectorizer()"
      ],
      "metadata": {
        "id": "i1nqRS7raunM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "clf1 = MultinomialNB()"
      ],
      "metadata": {
        "id": "yngXfMGfavte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = Pipeline([('vectorizer',tvec)\n",
        "                 ,('classifier',clf1)])"
      ],
      "metadata": {
        "id": "SY0ZQJe2ax_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "j4w9LBh1a1tQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hasil1=model1.predict(x_test)"
      ],
      "metadata": {
        "id": "P1qOA0fLa5Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "cm = confusion_matrix(y_test,hasil1)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model1.classes_)\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SHTiPNpZIyqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = classification_report(y_test,hasil1)\n",
        "print('Classification report : \\n',matrix)"
      ],
      "metadata": {
        "id": "uj53mC1ba8Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTING\n",
        "def classify_text(input_text):\n",
        "    models = [\n",
        "        ('MultinomialNB', model_multinomialNaiveBayes)\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models:\n",
        "        prediction = model.predict([input_text])\n",
        "        results[name] = prediction[0]\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "gU_J8wzTbAKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_multinomialNaiveBayes = model1.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "2C727gldNPAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = input(\"Masukkan teks yang ingin diklasifikasikan: \")\n",
        "\n",
        "results = classify_text(input_text)\n",
        "\n",
        "print(\"Input kata :\", input_text)\n",
        "\n",
        "print(\"\\nHasil Klasifikasi:\")\n",
        "for model, prediction in results.items():\n",
        "    print(f\"{model}: {prediction}\")"
      ],
      "metadata": {
        "id": "XTZMPGIqbXv5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}